:imagesdir: ../assets/images
[#optimization-practice]
= vLLM & Performance Tuning

== Chatbot Case Study: Latency Optimization for granite-3.3-8b-instruct with vLLM

Let's apply our knowledge to a concrete scenario: you need to serve granite-3.3-8b-instruct that is integrated with a chat application. Your primary goal is to minimize inference latency pass:[<span title="Latency: The time it takes for a single request to complete." style="cursor: help;">&#128161;</span>] for a given number of concurrent users -32 in this case- with an expected generation length <2048 tokens. 
As you have limited flexibility to scale GPU resources, you want to understand the key vLLM parameters and strategies to maximize performance before simply adding more hardware.

== Introduction

Before diving into tuning, it's important to understand the key performance considerations in vLLM:

*Latency*: The time it takes for a single request to complete. Of particular importance here is going to be tracking the Time To First Token - *TTFT* pass:[<span title="TTFT: How quickly the user sees the first word of the response." style="cursor: help;">&#128161;</span>], which will ensure the user can experience snappier responses.

*Throughput*: The number of requests (or tokens) processed per unit of time. While we're optimizing for latency, higher throughput often means better resource utilization, which can indirectly help avoid latency spikes due to queuing.

. First we'll need to re-deploy the granite-3.3-8b-instruct. Open a terminal and login to your OpenShift cluster. Navigate to our **llm-hosting** project:
+
[source,sh,role=execute]
----
oc project llm-hosting
----

. We'll be re-deploying granite-3.3-8b-instruct through the vllm-kserve helm chart.

. Open the `workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml` file to view the new arguments we will pass to the deployment. Your new file looks like the one below to facilitate the optimization exercises:

+
[source,yaml]
----
fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=2048"

image:
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm'

  # -- The tag or sha for the model server image
  tag: rhoai-2.22-cuda-5e9c649953464aa3a668aba1774e42fc933f721b

  # -- The vLLM version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""

  # -- Resource configuration for the vLLM container
resources:
  requests:
    cpu: '2'
    memory: 8Gi
    nvidia.com/gpu: '1'
  limits:
    cpu: '2'
    memory: 16Gi
    nvidia.com/gpu: '1'

# Disable external route creation to avoid NetworkPolicy issues
endpoint:
  externalRoute:
    enabled: false

# Move vLLM args to ServingRuntime so they actually work
servingRuntime:
  args:
    - "--port=8080"
    - "--model=/mnt/models"

# -- The tolerations to be applied to the model server pod.
tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    value: "True"
    operator: Equal

----
+
*--disable-log-requests* is used to prevent the logging of detailed request information. By default, vLLM logs various details about each incoming request, which can be useful for debugging and monitoring. However, in production environments or when high performance is critical, excessive logging can consume resources and generate large log files. The --disable-log-requests flag addresses this by turning off the detailed logging of individual requests.
+
*--max-num-seqs* configuring the server to handle a maximum of 32 concurrent requests in a single parallel computation step (a batch). For example, if 32 different users send a prompt to your chatbot at the same time.
+
*--max-model-len* argument sets the maximum number of tokens that the vLLM server will allow for a single sequence. This total includes both the user's prompt and the generated response combined.

. Now that we've set the engine arguments we'll go ahead and re-deploy the granite model with Helm. We will uninstall and then reinstall the helm deployment

+
[source,sh,role=execute]
----
helm uninstall granite-8b
----
+

+
[source,sh,role=execute]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  -f workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n llm-hosting
----
+
[TIP]
====
If you didn't add the redhat-ai-services helm chart repository (you should have done this in module 1.1) to your local helm client, you can do so by running the following command:

[source,console,role=execute,subs=attributes+]
----
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
----

Additionally, if you did not add all of the necessary pipeline manifests from Module 2 into your cluster, please do so now:

[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/guidellm-pipeline/upload-results-task.yaml -n llm-hosting
oc apply -f workshop_code/guidellm-pipeline/guidellm-pipeline.yaml -n llm-hosting
oc apply -f workshop_code/guidellm-pipeline/pvc.yaml -n llm-hosting
oc apply -f workshop_code/guidellm-pipeline/guidellm-benchmark-task.yaml -n llm-hosting
oc apply -f workshop_code/guidellm-pipeline/mino-bucket.yaml -n llm-hosting
----
====

. Login to OpenShift AI and go to your *LLM Host* Data Science Project. Wait until the model fully deploys (green check) before continuing. 
+
image::granite-deployed.png[]
// TO DO: Identify areas in this section that need to go to the previous.
. We'll be running an OpenShift pipeline with GuideLLM to benchmark our model as we step through the optimizations. If you have not already deployed the GuideLLM pipeline, ensure you follow the steps from module 2 to do so.

. Lets run the GuideLLM pipeline! Open the workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml file. Let's go through some of the important GuideLLM arguments we're setting. The pipeline will run a benchmark for 1, 4, 8, and 16 concurrent users. 
+
image::pipelinerun.png[pipelinerun]
+
1. *target* -  Specifies the target path for the backend to run benchmarks against. +
2. *model-name* - Allows selecting a specific model from the server. If not provided, it defaults to the first model available on the server. Useful when multiple models are hosted on the same server. +
3. *processor* - To calculate performance metrics like tokens per second, the benchmark script needs to count how many tokens were generated. To do this accurately, it must use the exact same tokenizer that the model uses. Different models have different tokenizers. 
4. *data-config* -  Specifies the dataset to use. This can be a HuggingFace dataset ID, a local path to a dataset, or standard text files such as CSV, JSONL, and more. Additionally, synthetic data configurations can be provided using JSON or key-value strings. Synthetic data options include:
+
* prompt_tokens: Average number of tokens for prompts.
* output_tokens: Average number of tokens for outputs.
* TYPE_stdev, TYPE_min, TYPE_max: Standard deviation, minimum, and maximum values for the specified type (e.g., prompt_tokens, output_tokens). If not provided, will use the provided tokens value only.
* samples: Number of samples to generate, defaults to 1000.
* source: Source text data for generation, defaults to a local copy of Pride and Prejudice.
+
5. *rate-type* - Defines the type of benchmark to run (default sweep). Supported types include:
+
* synchronous: Runs a single stream of requests one at a time. --rate must not be set for this mode.
* throughput: Runs all requests in parallel to measure the maximum throughput for the server (bounded by GUIDELLM__MAX_CONCURRENCY config argument). --rate must not be set for this mode.
* concurrent: Runs a fixed number of streams of requests in parallel. --rate must be set to the desired concurrency level/number of streams.
* constant: Sends requests asynchronously at a constant rate set by --rate.
* poisson: Sends requests at a rate following a Poisson distribution with the mean set by --rate.
* sweep: Automatically determines the minimum and maximum rates the server can support by running synchronous and throughput benchmarks, and then runs a series of benchmarks equally spaced between the two rates. The number of benchmarks is set by --rate (default is 10).
+
6. *rate* - request rate.

. Review the file to ensure all the parameters look accurate and then run the pipeline:
+
[source,sh,role=execute]
----
oc create -f workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml -n llm-hosting
----

. Go to your OpenShift console and view the pipeline run under your **llm-hosting** project. Click on the bar under *Task Status*.
+
image::ocp_pipelinerun_list.png[]
+
Note the two tasks in the pipeline. The first task runs the GuideLLM benchmark and saves the results to a PVC. The second task uploads the results to an s3 bucket. Feel free to click on each task to view the log files.
+
If the pipeline finishes successfully the tasks will have green check marks like the ones in the image below. 
+
image::ocp_pipelinerun_details.png[pipelinerun details in ocp]
+
NOTE: We'll view the results of the benchmark in the next section, but feel free to access your Minio instance and look at the files in your minio bucket. 

. Go to OpenShift AI and create a workbench with a *Standard Data Science* notebook. Everything else can be let as the default configuration.
+
NOTE: Feel free to use an existing workbench if you already have one running from previous exercises. 

. Open your workbench once it starts up and upload all of the files under *guidellm_notebook_charts*. These are in the repo files you cloned. It might be easier to download these to your local computer and the upload them through the OpenShift AI web ui.
+
image::upload_notebooks.png[upload notebooks]

. Open the *graph_benchmarks.ipynb* notebook and run the first cell. This will download the latest benchmark file from s3 that was created and uploaded when we ran the most recent pipeline. 
+
image::download_benchmark_results.png[downloading benchmark results from s3]

. You should now see a *benchmark_<TIMESTAMP>.txt* file. This should be the latest time stamped benchmark in the s3 bucket. Open that file and in the top toolbar go to _View -> Wrap Words_ so you the file is easier to read. 
+
Review the results. You can see the number of sequences under metadata benchmark and the related median TTFT. 
+
image::benchmark_results.png[benchmark results]

. Run the next cell to extract the Metadata benchmark and the median TTFTs.
+
image::extract_results.png[extract benchmark results]

. Finally, run the last cell in the notebook to graph the median TTFT per number of sequences. All reported times are in milliseconds. Notice how quickly we exceed the "seconds" threshold with even a slight increase in concurrent users—serving LLMs is hard! NOTE: If you are using one L4 for this lab then the curve will be even flatter and have higher TTFT values. 
+
image::ttft_chart.png[TTFT chart]

. Rename the benchmark file as `benchmark_1.txt`. We'll use this file in a later exercise.
 

All reported times are in `ms`. All reported times are in milliseconds. Notice how quickly we exceed the "seconds" threshold with even a 
slight increase in concurrent users—serving LLMs is hard!  

== vLLM Tuning Strategies for Granite 3.3 8B Latency

Granite-3.3-8b-instruct is a popular, powerful small-size _dense_ model. Here are the primary avenues for optimization.

=== GPU Allocation & Batching Parameters: Managing Concurrency

For a "given amount of concurrent users," how you manage batching is critical to maximize GPU utilization without introducing excessive queueing latency.
Let's take a look at some of the most popular vllm configurations.

`--max-model-len`: The maximum sequence length (prompt + generated tokens) the model can handle.

*Goal*: Set this to the minimum _reasonable_ length for your use case. Too small means requests get truncated; too large means less space for KVCache, which will impact your performance.
At startup, vllm will profile the model using this value, as it needs to ensure it is able to serve at least one request with length=max-model-len.
This is also a trade-off with the next parameter, `max-num-seqs`.
Tuning: If most of your requests are short, keeping max-model-len tighter can allow more requests into the batch (by increasing `max-num-seqs`).

NOTE: `max-num-batched-tokens` is a highly related parameter. It's limiting the amount of tokens the scheduler can schedule, rather than what the model can produce.
So the actual number limiting the amount of memory allocated for the model runtime is actually `min(max-model-len, max-num-batched-tokens)`.

You can verify the impact of this parameter by increasing its value when starting vLLM and then observing the amount of memory reserved for KVCache.
Check out the logs for our starting config:

_An abbreviated version of our starting config listed here for reference only. No need to apply it again_
```
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=2048"
```

. Go to your OpenShift web console. Select the *llm-hosting* project and open the logs for the *granite-8b-predictor-00003-deploymentXXXXX* pod. 
+
image::granite-pod.png[Granite pod]
+
Note the KV cache size at the top of the log
+
[subs=+quotes]
----
INFO 08-26 19:08:24 [gpu_worker.py:227] *Available KV cache memory: 4.19 GiB*
INFO 08-26 19:08:24 [kv_cache_utils.py:715] GPU KV cache size: 27,488 tokens
----

. Now increase the model size to `--max-model-len 4096 --max-num-batched-tokens 4096`:
+
Open _workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml_ and change the model arguments as in the below configuration (everything else should remain unchanged):
+
[source,sh,role=execute]
----
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=4096"
    - "--max-num-batched-tokens=4096"
----
+
. Rerun the helm deployment

+
[source,sh,role=execute]
----
helm uninstall granite-8b
----

[source,sh,role=execute]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n llm-hosting
----

. After the model is redeployed take a look at the KV Cache size in the pod's log file (it will take a minute for the pod's logs to progress to show this data point). Note that the available KV Cache size is now slightly smaller than it was before due to the increased --max-model-len value.
+
[subs=+quotes]
----
INFO 08-26 20:01:28 [gpu_worker.py:227] *Available KV cache memory: 3.99 GiB*
INFO 08-26 20:01:28 [kv_cache_utils.py:715] GPU KV cache size: 26,112 tokens
----

`--max-num-seqs`: The maximum number of sequences (requests) that can be processed concurrently. This is often referred to as the batch size, allowing for higher throughput.

*Goal*: Set this to the minimum _reasonable_ length for your use case. When this is too high, your requests under load might get fractioned into smaller chunks resulting in higher end-to-end latency. If this is too low, you might be under-utilizing your GPU resources.

Let's see this case in practice. Modify the script to limit the number max requests to 1 and run the benchmark pipeline with 4 requests at a time.

. Update the _workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml_ with the `max-num-seqs` to 1.
+
[source,sh,role=execute]
----
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=1"
    - "--max-model-len=2048"
----

. Rerun the helm deployment
+

[source,sh,role=execute]
----
helm uninstall granite-8b
----

[source,sh,role=execute]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n llm-hosting
----

. After the model redeploys update the _/workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml_ *rate* to 4.0. 
+
image::benchmark_rate_4.png[set the rate to 4, align="left"]

. Rerun the guidellm benchmark pipeline.
+
[source,sh,role=execute]
----
oc create -f workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml -n llm-hosting
----

. After the pipeline finishes go to your OpenShift AI workbench and open the _graph_benchmarks.ipynb_ file. Execute the first cell to download the latest benchmark file.
+
image::cell1_notebook.png[download latest benchmark, align="left"]

. Open the *benchmark_1.txt* file and the latest benchmark file (*benchmark_<TIMESTAMP>.txt*) you just downloaded from Minio. Go to *View -> Wrap Words* so it's easier to read the files.
+
What is happening here is that the engine is effectively being throttled and is only executing one request at a time. This is over 6x slower!
+
.Latest benchmark file
image::benchmark-rate4seq1.png[most recent benchmark, align="left"]
+
.First benchmark file
image::benchmark1-rate4seq32.png[first benchmark, align="left"]
+
Also notice another important indicator of an unhealthy deployment from the logs. Note the 31 pending requests:
+
[subs=+quotes]
----
INFO 08-14 23:05:18 metrics.py:455] Avg prompt throughput: 152.5 tokens/s, 
Avg generation throughput: 14.3 tokens/s, 
Running: 1 reqs, Swapped: 0 reqs, *Pending: 31 reqs*, 
GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
----
+
Especially when coupled with high waiting time (`vllm:request_queue_time_seconds_sum` metric from `/metrics`). 
+
You can access the metrics by going to 
your https://<INFERENCE_ENDPOINT>/metrics in a browser.
+
[subs=+quotes]
----
vllm:request_queue_time_seconds_sum{model_name=
"granite-8b"} 35.21637320518494
----


== Model Quantization

Quantization is arguably the most impactful change you can make for latency, especially with vLLM's efficient kernel implementation for w8a16 or w4a16.

Why? Reducing precision directly shrinks the model's memory footprint and enables faster arithmetic on modern GPUs.

What to try (_highly_ dependent on available hardware):

FP8: If you have access to NVIDIA H100 GPUs or newer (e.g., B200), FP8 (E4M3 or E5M2) is a game-changer. These GPUs have dedicated FP8 Tensor Cores that 
offer significantly higher throughput compared to FP16. This provides a direct path to lower latency per token without significant accuracy loss 
for Llama 3 models.

INT8 (e.g., AWQ): Starting with A100 or even A6000/3090 GPUs, INT8 quantization is an excellent choice. It reduces the model to 8B * 1 byte = 8GB, 
halving the memory footprint and enabling faster integer operations. 

INT4: If you're pushing for absolute minimum latency and can tolerate a small accuracy trade-off, INT4 (e.g., via AWQ or other 4-bit methods) 
can reduce the model to 8B * 0.5 bytes = 4 GB. This is extremely memory-efficient and, on some hardware, can offer further speedups. 
Test accuracy thoroughly with your specific use case, as 4-bit can sometimes be more sensitive.
Similarly, check out FP4 versions when Nvidia Blackwell hardware is available.

[.table-scroll-wrapper]
--
[options="header"]
|===
| Quantization Type | Recommended Hardware | Key Benefits for Latency | Memory Footprint (for Llama 3 8B) | Accuracy Consideration | Notes

| **FP8 (E4M3/E5M2)**
| NVIDIA H100 (or newer)
| - Dedicated FP8 Tensor Cores for significantly higher throughput.
| 8B * 1 byte ~= 8 GB
| Minimal accuracy loss for Llama 3 models.
| Already a standard for high-performance inference.

| **INT8 (e.g., AWQ)**
| NVIDIA A100, A6000 (or newer)
| - Halves memory footprint.
| 8B * 1 byte ~= 8 GB
| Generally decent accuracy preservation.
| Widely supported (across manufacturers) and fast.

| **INT4 (e.g., AWQ)**
| NVIDIA A100, A6000 (or newer)
| - Extremely memory-efficient.
| 8B * 0.5 bytes ~= 4 GB
| Requires an accuracy trade-off.
| Pushes for absolute minimum latency.

| **FP4**
| NVIDIA Blackwell (B200)
| - New architecture support for even lower-precision floating-point.
| 8B * 0.5 bytes ~= 4 GB
| Designed to maintain better accuracy than integer 4-bit, but still requires validation.
| Emerging standard with the latest hardware (e.g., NVIDIA Blackwell). Look for NVFP4 variants.
|===
--

Please refer to the compatiblity chart https://docs.vllm.ai/en/latest/features/quantization/supported_hardware.html[^] for up to date quantization support in vLLM.

. Let us try to run a w8a8 int8 model with the original vLLM engine arguments we started with. Update the _workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml_ with the following values. Make sure you update the *uri* with the correct model. 

NOTE: Leave everything else below this section in the file unchanged.

+
[source,sh,role=execute]
----
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.1-8b-instruct-quantized.w8a8
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=2048"
----

. Rerun the helm deployment
+
[source,sh,role=execute]
----
helm uninstall granite-8b
----

[source,sh,role=execute]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n llm-hosting
----

. After the model redeploys (this will take a moment as you are pulling a new model car down) update the _/workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml_ *rate* to 1.0, 4.0, 8.0, 16.0. 
+
image::benchmark_rate_1_4_8_16.png[set the rate to 4, align="left"]

. Rerun the guidellm benchmark pipeline.
+
[source,sh,role=execute]
----
oc create -f workshop_code/guidellm-pipeline/guidellm-pipelinerun.yaml -n llm-hosting
----

. After the pipeline finishes go back to your OpenShift AI workbench and open the _graph_benchmarks.ipynb_ file. Execute the first cell to download the latest benchmark file.

. Copy and paste the code snippet below into the second cell or edit your code to be the same. This code extracts the median TTFT from our first benchmark run with the full weight Granite model and extracts the median TTFT from the most recent benchmark with the quantized version.
+
Execute the cell. 
+
[source,sh,role=execute]
----
#extract the Metadata benchmark and the median TTFTs
from parse_benchmark_stats import extract_ttft_from_file
data = extract_ttft_from_file('benchmark_1.txt')
data2 = extract_ttft_from_file(latest_file)
print(data)
print(data2)
----

. Copy and paste the code snippet below into the third cell or edit your code to be the same. This will generate a graph of the median TTFT for the poission rate for both models.
+
Execute the cell.
+
[source,sh,role=execute]
----
#graph of median TTFT vs poisson rate
%pip -q install seaborn
from seaborn_graph import create_ttft_plot
create_ttft_plot(data, data2, 'granite-3.3-8b-instruct', 'granite-3.1-8b-instruct-quantized.w8a8')
----

Your chart should look similar to the one below.

image::quant_vs_unquant.png[quant_vs_unquantized]

Up to 2x speedup!

== Using a smaller model 

Following the same principle as quantization, serving a smaller model (when accuracy on task is acceptable) will enable faster response
times as less data is moved around (model weights+activations) and less sequential computations are involved (generally fewer layers).
For this particular use-case, consider `ibm-granite/granite-3.1-2b-instruct`.


=== Using a different model

While Granite 3 is a strong dense model, for certain latency-sensitive scenarios, considering a Mixture-of-Experts (MoE) model like Mixtral 8x7B could be a 
compelling alternative.

Why MoE for Latency? MoE models have a large total number of parameters (e.g., Mixtral 8x7B has 47B total parameters), but critically, 
they only activate a sparse subset of these parameters (e.g., 13B for Mixtral 8x7B) for each token generated. 
This means the actual computational cost per token is significantly lower than a dense model of its total parameter count.
Which is especially true when sharding experts over multiple GPUs with MoE especially with vLLM's optimized handling of MoE sparsity. 

Trade-offs: While MoE models can offer lower inference latency per token due to their sparse activation, they still require enough GPU memory 
to load the entire model's parameters, not just the active ones. So, Mixtral 8x7B will demand more VRAM than Llama 3 8B,
even if it's faster per token. You'll need sufficient GPU memory (e.g., a single A100 80GB or multiple smaller GPUs with tensor parallelism) to fit the full 47B parameters.

vLLM has strong support for MoE models like Mixtral, including optimizations for their unique sparse compute patterns and dynamic routing.

Consider When: Your application might benefit from the increased quality often associated with larger (total parameter) MoE models, combined with the per-token speed advantages 
of their sparse computation.


== Speculative Decoding.

Speculative decoding is a powerful technique to reduce the generation latency, particularly noticeable for the Time To First Token (TTFT).
Speculative decoding is fundamentally a tradeoff: spend a little bit of extra compute to reduce memory movement.
The extra compute is allocated towards the smaller draft model and consequent proposer verifying step.
At low request rates, we are memory-bound, so reducing memory movement can really help with latency. 
However, at higher throughputs or batch sizes, we are compute-bound, and speculative decoding can provide worse performance. 

image::spec_dec.png[spec_dec]

The graph here from https://developers.redhat.com/articles/2025/07/01/fly-eagle3-fly-faster-inference-vllm-speculative-decoding#speculative_decoding__a_solution_for_faster_llms
highlighs the tradeoffs of speculative decoding at low request rate vs bigger batch sizes.
Take away message: as long as the number of requests is bound to use a non-intensive amount of GPU resources (lower req/s), spec decoding can provide
a nice speedup.

NOTE: Speculative decoding in vLLM is not yet fully optimized and does not always yield intended inter-token latency reductions. In particular in this case it will fallback to V0 due to
V1 still not supporting this particular speculation technique. Mind that what we're comparing here is not going to be exactly apples to apples, as the V0 and V1 engine have quite
substantial architectural differences. 

What to try: You'll need to specify a smaller draft model. A good starting point for Llama/granite might be a smaller Llama/granite variant or as in this 
example a speculator trained specifically for our use-case. 

NOTE: We are working on OpenShift AI instructions for speculative decoding with engineering.


// TO DO: Address Speculative decoding in RHOAI
// Let's change the vllm startup command:

// ```bash
// VLLM_CMD="vllm serve $MODEL --max-num-seqs $MAX_NUM_SEQS --max-model-len 2048 --enable-chunked-prefill --max-num-batched-tokens 2048  --speculative-config\
//  '{\"model\": \"ibm-granite/granite-3.0-8b-instruct-accelerator\", \"num_speculative_tokens\": 4, \"draft_tensor_parallel_size\": 1}' &"
// ```

// +
// [source,sh,role=execute]
// ----
// deploymentMode: RawDeployment

// fullnameOverride: granite-8b
// model:
//   modelNameOverride: granite-8b
//   uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct-accelerated
//   args:
//     - "--disable-log-requests"
//     - "--max-num-seqs=32"
//     - "--max-model-len=2048"
//     - "--max-num-batched-tokens=2048"
//     - "--enable-chunked-prefill"
//     - "--speculative_config={num_speculative_tokens: 4, draft_tensor_parallel_size: 1}"
// ----

// . Rerun the helm deployment
// +
// [source,sh,role=execute]
// ----
// helm upgrade -i granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
//   --values workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml -n vllm
// ----

// vLLM will spin up an instance with the two models. 
// There's no free-lunch though, mind that the GPU memory will now be comprised of: the original `ibm-granite/granite-3.1-8b-instruct` weights + `ibm-granite/granite-3.0-8b-instruct-accelerator` proposer weights
//  + a KV cache for *both* models.

// image::spec.png[specized]


// A key metric to keep an eye on when serving a speculator is the `acceptance rate`:
// ```
// INFO 07-17 11:11:38 [metrics.py:439] Speculative metrics: Draft acceptance rate: 0.381, System efficiency: 0.427, Number of speculative tokens: 3, Number of accepted tokens: 3781, Number of draft tokens: 9930, Number of emitted tokens: 5657.
// ```

// This is the percentage of tokens being produced by the speculator that match the ones of the draft model.
// Here we're still on the lower side as ideally you would want to see this number be higher.

// This is tied to major drawback holding back the adoptability of speculative decoding, which is the fact that the speculator needs to be trained specifically for the model you intend to deploy,
// in order to achieve an high acceptance rate.
// Being a data-dependent technique, this is mostly useful when it is 

== Final Notes

Optimization is an iterative process. As you tune vLLM, continuously monitor key metrics:

- Time To First Token (TTFT): Critical for interactive applications.
- Throughput (Tokens/sec or Requests/sec): To ensure your concurrency goals are met.
- GPU Utilization: High utilization indicates efficient use of resources.
- GPU KV cache usage: At very high rates early on into a benchmark, it is an indicator of likely insufficient memory for KV cache.

Important engine arguments
- https://docs.vllm.ai/en/v0.9.0/usage/usage_stats.html
- VLLM_NO_USAGE_STATS=1
- DO_NOT_TRACK=1

////
[#configuration]
=== vLLM Configuration

* Sizing KV Cache for GPUs - https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/model_sizing/index.html[^]
** Configuring --max-model-length
**  KV Cache Quantization
*** --kv-cache-dtype
* vLLM configuration/optimization best practices
** --served-model-name
** --tensor-parallel-size
** --enable-expert-parallel
** --gpu-memory-utilization
** --max-num-batched-tokens
** --enable-eager
** --limit-mm-per-prompt
* Configuring tool calling
* Configuring speculative decoding
* prefill
* TTFT
* Intertoken Latency
* Accuracy vs Latency
* Int vs Floating point
* Model Architecture and GPU Architecture
* Tuning/configuring vLLM
* Performance analysis
////