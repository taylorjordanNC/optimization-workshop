:imagesdir: ../assets/images
[#eval-accuracy]
# Evaluating Model Accuracy with Trusty AI lm-eval-harness service

While performance metrics like latency and throughput are critical for deploying efficient GenAI systems, **task-level accuracy and reasoning quality** are equally essential for selecting or fine-tuning a model. In this activity, we use the lm-eval-harness framework to evaluate how well a language model performs across established benchmarks, focusing on reasoning and subject matter understanding.

## What is Trusty AI?

https://trustyai.org/docs/main/main[TrustyAI^] is an open-source AI explainability and trustworthiness platform designed to help developers and data scientists understand and monitor their machine learning models. It provides tools to analyze predictions, identify biases, and ensure that AI systems are fair, transparent, and reliable. As part of its comprehensive toolkit, TrustyAI integrates the popular lm-eval harness to specifically benchmark and evaluate the performance of large language models against standardized tests, allowing users not only to understand why a model makes a decision but also to quantitatively measure its accuracy and capabilities. This combination of explainability and performance evaluation enables organizations to build more responsible, ethical, and robust AI applications.

## What is lm-eval-harness?

**lm-eval-harness** is a community-maintained benchmarking toolkit from **EleutherAI**. It enables consistent, reproducible evaluation of large language models (LLMs) across dozens of academic and real-world benchmarks, such as:

* MMLU (Massive Multitask Language Understanding)

* HellaSwag, ARC, and Winogrande

* Question answering, common sense reasoning, reading comprehension, and more

The framework supports both open-source models and OpenAI-compatible endpoints, and can be customized with additional tasks, prompt templates, and evaluation metrics.

## ARC-Easy

Today we will be running the ARC-Easy evaluation. 

**ARC-Easy** (AI2 Reasoning Challenge - Easy) is a **science reasoning benchmark** that tests a model's ability to answer elementary and middle-school level science questions. Part of the broader https://allenai.org/data/arc[ARC dataset^], ARC-Easy contains **multiple-choice science questions** that require basic scientific knowledge and reasoning skills. It's designed to test a model's **factual knowledge, logical reasoning, and comprehension abilities** in scientific domains—essential capabilities for enterprise AI applications that need reliable, factual responses.

## Today's Activity

In this section of our lab we will:

. Set up the Trusty AI operator
. Create and run the lm-eval job
. Interpret and understand results

### Setup Trusty AI

. Fist, we'll change the TrustyAI managementState from "Removed" to "Managed" in the default DataScienceCluster. This activates the TrustyAI operator resource.
+
[source,console,role=execute,subs=attributes+]
----
oc patch datasciencecluster default-dsc -p '{"spec":{"components":{"trustyai":{"managementState":"Managed"}}}}' --type=merge
----
+

. Configure TrustyAI to allow downloading remote datasets from Huggingface
+
By default, TrustyAI prevents evaluation jobs from accessing the internet or running downloaded code.
A typical evaluation job will download two items from Huggingface:
+
- The dataset of the evaluation task, and any dataset processing code
- The tokenizer of your model 

+
If you trust the source of your dataset and tokenizer, you can override TrustyAI's default setting.
In our case, we'll be downloading:

- [allenai/ai2_arc](https://huggingface.co/datasets/allenai/ai2_arc) +
- [Phi-3-mini-4k-instruct's tokenizer](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

+
To download those two resources, run:

+
[source,console,role=execute,subs=attributes+]
----
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications  \
--type merge -p '{"metadata": {"annotations": {"opendatahub.io/managed": "false"}}}'
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'
oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications
----

+
Wait for your `trustyai-service-operator-controller-manager` pod in the `redhat-ods-applications` namespace
to restart, and then TrustyAI should be ready to go.

. Ensure Granite model is configured
+
Add the external endpoint to the *workshop_code/evals/trusty/arc_easy.yaml* file in the following section (it is likely if you have followed the lab to this point that this will already be correct in your yaml):
+
[source,console,role=execute,subs=attributes+]
----
- name: base_url
      value: https://<YOUR_EXTERNAL_INFERENCE_ENDPOINT>/v1/completions # the location of your model's /chat/completions or /completions endpoint
----

### Create and run the lm-eval job

. Run the evaluation
+
To start an evaluation, apply an `LMEvalJob` custom resource as defined in the following file (it may take a few minutes, use and oc describe command of the pod if you're worried):
+
[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/arc_easy.yaml -n llm-hosting
----
+
Check out the arc_easy.yaml file to learn more about the `LMEvalJob` specification.
+
If everything has worked, you should see a pod called `arc-easy-eval-job` running in your namespace. 
You can watch the progress of your evaluation job (once the pod is finished being created) by running:
+
[source,console,role=execute,subs=attributes+]
----
oc logs -f arc-easy-eval-job -n llm-hosting
----
+
You will see progression in percentage points.
+
Or alternatively the logs of the model pod:
+
[source,console,role=execute,subs=attributes+]
----
oc logs -f granite-8b-predictor-<exact-pod-name> -n llm-hosting
----

You will see the exact questions getting passed to the model endpoint!

If you use the OpenShift console to look at the job pod logs, you'll see this:

image::arc-eval-progress.png[]

This evaluation run will take approximately 10 minutes.

#### While You Wait: What is lm-eval-harness?

* Check out this https://github.com/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb[overview notebook^] to explore extensibility and task definitions.

* View real Red Hat https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437[validated model results^] to understand benchmark outcomes in production contexts and to see how your favorite models rank.

### Interpret and understand results

. Interpreting MMLU-Pro Results
+
**Accuracy**: The primary metric is multiple-choice accuracy, indicating how often the model selects the correct answer from 10 options.
+
~10% = random guessing baseline
+
~30-50% = typical for smaller or untuned models
+
~60-70%+ = high reasoning capability or fine-tuned performance
+
**Per-subject Scores**: Breakdowns by subject (e.g., philosophy, law, computer science) help identify a model's strengths and weaknesses in specific domains.
+
**Implications**: Higher MMLU-Pro accuracy generally correlates with better real-world task generalization, especially for tasks involving structured inputs, knowledge retrieval, and logic.

. Check out the results
+
After the evaluation finishes (it took about 8.5 minutes on our test cluster), you can take a look at the results. These are stored in the `status.results` field of the LMEvalJob resource:
+
[source,console,role=execute,subs=attributes+]
----
oc get LMEvalJob arc-easy-eval-job -n llm-hosting -o jsonpath='{.status.results}' | jq '.results'
----
+
returns:
+
[source,console]
----
{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.8186026936026936,
    "acc_stderr,none": 0.007907153952801706,
    "acc_norm,none": 0.7836700336700336,
    "acc_norm_stderr,none": 0.00844876352205705
  }
}
----
+
*Explanation of results*
+
*acc,none*: This stands for accuracy. The value 0.8186 means the model answered approximately 81.86% of the questions correctly based on its raw output.
+
*acc_stderr,none*: This is the standard error of the accuracy. The value 0.0079 represents the margin of error for the accuracy score. It indicates how much the result might vary if the test were run again. A smaller number means the result is more statistically reliable.
+
*acc_norm,none*: This is the normalized accuracy. The value 0.7836 means that after cleaning up the model's answers (e.g., removing extra spaces, punctuation, or standardizing capitalization), it answered about 78.37% of the questions correctly. This score is often considered a more realistic measure of performance.
+
*acc_norm_stderr,none*: This is the standard error for the normalized accuracy, indicating the margin of error for that specific score.
+
Now you're free to play around with evaluations! You can see the full list of evaluation supported by 
lm-evaluation-harness https://github.com/red-hat-data-services/lm-evaluation-harness/blob/main/lm_eval/tasks/README.md[here^]
+
*More information*
+
[TrustyAI Notes Repo](https://github.com/trustyai-explainability/reference/tree/main)
+
[TrustyAI Github](https://github.com/trustyai-explainability)



// TODO:### Testing your model with a custom dataset 

// TODO:### Testing your model in a disconnected environment

## Summary

What We Did:

* Set up TrustyAI operator - Enabled model evaluation framework in OpenShift AI
* Configured internet access - Allowed downloading of evaluation datasets from HuggingFace
* Connected to deployed model - Linked evaluation job to the Granite 8B inference service
* Ran ARC Easy benchmark - Tested model's reasoning on grade-school science questions
* Analyzed results - Achieved 81.8% accuracy, indicating strong reasoning performance

Key Outcome:

* ✅ Successfully evaluated deployed AI model accuracy using industry-standard benchmarks through TrustyAI + lm-eval-harness

Tools Used:

* TrustyAI: Enterprise evaluation operator
* lm-eval-harness: Standard benchmarking framework
* ARC Easy: Science reasoning benchmark
* Bottom Line: Demonstrated how to measure and validate AI model accuracy in production using automated evaluation pipelines.
