:imagesdir: ../assets/images

[#deploy-rhoai]
# Deploy vLLM on OpenShift AI with kserve and model car


Let's take a look at our first provided custom values file for this workshop(workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml):

[source,console,subs=attributes+]
----
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--max-model-len=1024"
    - "--enable-auto-tool-choice"
    - "--tool-call-parser=granite"
    - "--chat-template=/app/data/template/tool_chat_template_granite.jinja"

image:
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm'

  # -- The tag or sha for the model server image
  tag: rhoai-2.22-cuda-5e9c649953464aa3a668aba1774e42fc933f721b

  # -- The vLLM version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""
----

NOTE: The max model length for this deployment has been reduced to run cleanly on an single L4.  Just know that a typical deployment would likely have a large value associated with the max model length.

You may substitute the modelcar for a different model and adjust the arguments as desired using vLLM standard args: https://docs.vllm.ai/en/stable/configuration/engine_args.html#named-arguments. 

View our available modelcars here: https://quay.io/repository/redhat-ai-services/modelcar-catalog?tab=tags

If you choose to use a different model you'll need to ensure to change the tool-call-parser and the chat-template fields appropriately. 

If this is your first time with this lab it is probably best to stick with the standard model deployment provided.

## Understanding Chat Templates and Tool Calling

When using different models, you need to update these key arguments:

== Chat Templates
Chat templates format conversations for the model. Each model expects a specific format based on its training, and may support multiple different template types:

* **Jinja2 Templates**: Nearly all HF chat models ship with this template type. It's declaritive, portable and can be bundled in the model card and config without shipping additional code.
* **Pythonic Templates**: Requires shipping actual Python code. A bit less portable and requires more maintenance.

All models almost always ship with the jinja2 format template. Some models also include the pythonic templates but it isn't guaranteed. 

NOTE: For production deployments and advanced pipelines, the pythonic template is considered the standard and is recommended over the jinja2 template.

== Tool Call Parsers
For function calling models, the parser extracts structured tool calls from model output:

* `--tool-call-parser=granite` - For IBM Granite models

**Quick Reference:**
- Chat templates: https://docs.vllm.ai/en/stable/features/tool_calling.html
- Tool parsers: https://docs.vllm.ai/en/latest/cli/index.html#-tool-call-parser
- Use `pythonic` chat template when available for simpler configuration

## install jq for an easier time with json
[source,console,role=execute]
----
sudo dnf install -y jq
----

## Deploy the vLLM model car chart

### install helm on bastion machine

[source,console,role=execute]
----
curl -LO https://get.helm.sh/helm-v3.16.2-linux-amd64.tar.gz
tar -zxvf helm-v3.16.2-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm
helm version
rm -rf linux-amd64 helm-v3.16.2-linux-amd64.tar.gz
----

### Add the redhat-ai-services helm chart repository

[source,console,role=execute,subs=attributes+]
----
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
----

### Deploy helm chart in llm-hosting namespace

Run the following commands to deploy our vLLM-kserve helm chart. The chart version we are using is published, but we will be deploying it from our cloned repository so that we may view files and make any changes if desired.

OPTIONAL: Before deploying, adjust your `values.yaml` file as you desire as described in above sections or use the provided starting file.

Switch to the **llm-hosting** project: 

[source,console,role=execute,subs=attributes+]
----
oc new-project llm-hosting
----

NOTE: Ensure you're in the **etx-ai-presales** cloned repository in your local environment before continuing.

[source,console,role=execute,subs=attributes+]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  -f workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml 
----

we can watch the status with the command...
[source,console,role=execute]
----
oc get pods -n llm-hosting -w
----

## Verify deployment

It will take about 10 minutes for the model deployment to be ready.

* Login to OpenShift AI and go to the **LLM HOST** Data Science Project. Wait until the model fully deploys (green check) before continuing. 

image::granite-deployed-rhoai.png[Granite deployed on RHOAI]

You may then use your preferred method(s) to verify the successful deployment. We are not exposing an external route and the llm-hosting namespace within which we deployed the model has a network policy that blocks traffic from other namespaces. We will use a pod to curl the model.

### Curl Models

Again, in the OpenShift console terminal:

[source,sh,role=execute]
----
oc -n llm-hosting run curl --image=curlimages/curl --restart=Never -it --rm -- \
  curl -s http://granite-8b-predictor.llm-hosting.svc.cluster.local:8080/v1/models | jq .
----

## Conclusion

We now have our model car deployed and will move on to model optimization and evaluation!