apiVersion: v1
kind: ConfigMap
metadata:
  name: guidellm-config
data:
  # Default configuration for llama32-3b
  llama32-3b.yaml: |
    TARGET: "http://llama32-3b.llama-serve.svc.cluster.local:8000/v1"
    MODEL_NAME: "llama32"
    PROCESSOR: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    DATA_CONFIG: '{"type":"emulated","prompt_tokens":512,"output_tokens":128}'
    OUTPUT_FILENAME: "llama32-3b.yaml"
    RATE_TYPE: "synchronous"
    MAX_SECONDS: "1800"
  
  # Configuration for stress test
  stress-test.yaml: |
    TARGET: "http://llama32-3b.llama-serve.svc.cluster.local:8000/v1"
    MODEL_NAME: "llama32-stress"
    PROCESSOR: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    DATA_CONFIG: '{"type":"emulated","prompt_tokens":1024,"output_tokens":256}'
    OUTPUT_FILENAME: "llama32-stress.yaml"
    RATE_TYPE: "synchronous"
    MAX_SECONDS: "3600"
  
  # Configuration for async test
  async-test.yaml: |
    TARGET: "http://llama32-3b.llama-serve.svc.cluster.local:8000/v1"
    MODEL_NAME: "llama32-async"
    PROCESSOR: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    DATA_CONFIG: '{"type":"emulated","prompt_tokens":512,"output_tokens":128}'
    OUTPUT_FILENAME: "llama32-async.yaml"
    RATE_TYPE: "poisson"
    MAX_SECONDS: "1800"