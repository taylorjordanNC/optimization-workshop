apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: upload-guidellm-benchmark-results
spec:
  description: Upload guidellm benchmark results to s3 bucket
  params:
    - name: minio-api-route
      type: string
      description: s3 API endpoint
      default: "http://minio-service.showcase.svc.cluster.local:9000"
    - name: user
      type: string
      description: s3 username
      default: "minio"
    - name: password
      type: string
      description: s3 password
      default: "minio123"
    - name: bucket
      type: string
      description: s3 bucket name
      default: "guidellm-benchmark"
  steps:
    - name: upload-benchmark
      image: registry.access.redhat.com/ubi9/python-311
      workingDir: $(workspaces.shared-workspace.path)
      computeResources: {}
      env:
        - name: REQUESTS_CA_BUNDLE
          value: /etc/ssl/certs/ca-bundle.crt
        - name: SSL_CERT_FILE
          value: /etc/ssl/certs/ca-bundle.crt

        # Endpoint (make param win under multiple common env keys)
        - name: S3_ENDPOINT_URL
          value: $(params.minio-api-route)
        - name: AWS_ENDPOINT_URL
          value: $(params.minio-api-route)
        - name: AWS_S3_ENDPOINT
          value: $(params.minio-api-route)
        - name: AWS_S3_FORCE_PATH_STYLE
          value: "1"

        # Credentials
        - name: AWS_ACCESS_KEY_ID
          value: $(params.user)
        - name: AWS_SECRET_ACCESS_KEY
          value: $(params.password)

        # Bucket
        - name: S3_BUCKET
          value: $(params.bucket)

        # (optional) keep PARAM_* for backward compatibility
        - name: PARAM_S3_ENDPOINT_URL
          value: $(params.minio-api-route)
        - name: PARAM_S3_USERNAME
          value: $(params.user)
        - name: PARAM_S3_PASSWORD
          value: $(params.password)
      script: |
        #!/usr/bin/env python3
        import os, glob, sys
        # Install dependencies quietly
        os.system('pip install -q boto3')
        import boto3
        from botocore.client import Config

        # Resolve endpoint and creds from env with precedence & fallbacks
        S3_ENDPOINT_URL = (
            os.getenv("S3_ENDPOINT_URL")
            or os.getenv("AWS_ENDPOINT_URL")
            or os.getenv("AWS_S3_ENDPOINT")
            or os.getenv("PARAM_S3_ENDPOINT_URL")
            or "http://minio-service.showcase.svc.cluster.local:9000"
        )
        S3_USERNAME = (
            os.getenv("AWS_ACCESS_KEY_ID")
            or os.getenv("S3_USERNAME")
            or os.getenv("PARAM_S3_USERNAME")
            or "minio"
        )
        S3_PASSWORD = (
            os.getenv("AWS_SECRET_ACCESS_KEY")
            or os.getenv("S3_PASSWORD")
            or os.getenv("PARAM_S3_PASSWORD")
            or "minio123"
        )
        S3_BUCKET = os.getenv("S3_BUCKET") or "guidellm-benchmark"

        print(f"Using endpoint: {S3_ENDPOINT_URL}")
        print(f"Using user: {S3_USERNAME}")
        print(f"Using bucket: {S3_BUCKET}")

        def upload_files_to_s3(bucket_name, search_directory, file_pattern):
            """
            Finds all files matching a pattern in a directory and uploads them to an S3 bucket.
            It explicitly skips directories.
            """
            try:
                s3_client = boto3.client(
                    "s3",
                    endpoint_url=S3_ENDPOINT_URL,
                    aws_access_key_id=S3_USERNAME,
                    aws_secret_access_key=S3_PASSWORD,
                    config=Config(s3={"addressing_style": "path"}),
                    region_name="us-east-1",
                )
            except Exception as e:
                print(f"Failed to create S3 client: {e}")
                sys.exit(1)

            search_path = os.path.join(search_directory, file_pattern)
            print(f"Searching for items with pattern: {search_path}")
            found_items = glob.glob(search_path)

            if not found_items:
                print("No items found matching the pattern.")
                return

            print(f"Found {len(found_items)} item(s). Starting upload process...")

            for item_path in found_items:
                if os.path.isfile(item_path):
                    try:
                        s3_object_key = os.path.basename(item_path)
                        print(f"  Uploading '{item_path}' to bucket '{bucket_name}' as '{s3_object_key}'...")
                        s3_client.upload_file(item_path, bucket_name, s3_object_key)
                        print(f"Upload successful for: {item_path}")
                    except Exception as e:
                        print(f"Error uploading file {item_path}: {e}")
                else:
                    print(f" Skipping '{item_path}' because it is a directory.")

        if __name__ == "__main__":
            LOCAL_DIRECTORY = "/workspace/shared-workspace/"
            try:
                with open(os.path.join(LOCAL_DIRECTORY, "timestamp.txt"), "r") as f:
                    timestamp = f.read().strip()
                print(f"Using timestamp '{timestamp}' to find files.")
                WILDCARD_PATTERN = f"*{timestamp}*"
                upload_files_to_s3(S3_BUCKET, LOCAL_DIRECTORY, WILDCARD_PATTERN)
            except FileNotFoundError:
                print("Error: timestamp.txt not found. Cannot determine files to upload.")
            except Exception as e:
                print(f"An unexpected error occurred: {e}")
  workspaces:
    - name: shared-workspace
      description: Shared workspace for storing benchmark results
