deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--max-model-len=8192"
    - "--gpu-memory-utilization=0.95"
    - "--enable-auto-tool-choice"
    - "--tool-call-parser=granite"
    - "--chat-template=/app/data/template/tool_chat_template_granite.jinja"

image:
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm'

  # -- The tag or sha for the model server image
  tag: rhoai-2.22-cuda-5e9c649953464aa3a668aba1774e42fc933f721b

  # -- The vLLM version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""

# -- Resource configuration for the vLLM container
resources:
  requests:
    cpu: '2'
    memory: 8Gi
    nvidia.com/gpu: '1'
  limits:
    cpu: '2'
    memory: 16Gi
    nvidia.com/gpu: '1'

# Disable external route creation to avoid NetworkPolicy issues
endpoint:
  externalRoute:
    enabled: false

# Move vLLM args to ServingRuntime so they actually work
servingRuntime:
  args:
    - "--port=8080"
    - "--model=/mnt/models"

# -- The tolerations to be applied to the model server pod.
tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists